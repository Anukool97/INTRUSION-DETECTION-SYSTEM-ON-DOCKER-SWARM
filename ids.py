# -*- coding: utf-8 -*-
"""IDS

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aZjojJ1gIDHvv5-x5qQHsnDULZTPXeTs
"""

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

!nvidia-smi

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

downloaded = drive.CreateFile({'id':'1wSA5BaNLk5-nLISuJiMIJ_BdtsYEFf93'})
downloaded.GetContentFile('KDDTrain.txt')
downloaded = drive.CreateFile({'id':'1HBOQNE4KeOXg-jdFyLQrWw-Pd9hGbAaJ'})
downloaded.GetContentFile('KDDTest.txt')

import numpy as np
import pandas as pd
data_col_names=["duration","protocol_type","service","flag","src_bytes","dst_bytes","land","wrong_fragment",
                "urgent","hot","num_failed_logins","logged_in","num_compromised","root_shell","su_attempted",
                "num_root","num_file_creations","num_shells","num_access_files","num_outbound_cmds","is_host_login",
                "is_guest_login","count","srv_count","serror_rate","srv_serror_rate","rerror_rate","srv_rerror_rate",
                "same_srv_rate","diff_srv_rate","srv_diff_host_rate","dst_host_count","dst_host_srv_count",
                "dst_host_same_srv_rate","dst_host_diff_srv_rate","dst_host_same_src_port_rate",
                "dst_host_srv_diff_host_rate","dst_host_serror_rate","dst_host_srv_serror_rate",
                "dst_host_rerror_rate","dst_host_srv_rerror_rate","attack", "last_flag"]

df_kddtrain=pd.read_table("KDDTrain.txt", sep=",", names=data_col_names)
df_kddtrain = df_kddtrain.iloc[:,:-1]
df_kddtest=pd.read_table("KDDTest.txt",sep=",", names=data_col_names)
df_kddtest = df_kddtest.iloc[:,:-1]

df_kddtrain

mapping= {'ipsweep': 'Probe','satan': 'Probe','nmap': 'Probe','portsweep': 'Probe','saint': 'Probe','mscan': 'Probe',
        'teardrop': 'DoS','pod': 'DoS','land': 'DoS','back': 'DoS','neptune': 'DoS','smurf': 'DoS','mailbomb': 'DoS',
        'udpstorm': 'DoS','apache2': 'DoS','processtable': 'DoS',
        'perl': 'U2R','loadmodule': 'U2R','rootkit': 'U2R','buffer_overflow': 'U2R','xterm': 'U2R','ps': 'U2R',
        'sqlattack': 'U2R','httptunnel': 'U2R',
        'ftp_write': 'R2L','phf': 'R2L','guess_passwd': 'R2L','warezmaster': 'R2L','warezclient': 'R2L','imap': 'R2L',
        'spy': 'R2L','multihop': 'R2L','named': 'R2L','snmpguess': 'R2L','worm': 'R2L','snmpgetattack': 'R2L',
        'xsnoop': 'R2L','xlock': 'R2L','sendmail': 'R2L',
        'normal': 'Normal'
        }

df_kddtrain['attack_class'] = df_kddtrain['attack'].apply(lambda v: mapping[v])
df_kddtest['attack_class'] = df_kddtest['attack'].apply(lambda v: mapping[v])
df_kddtrain.drop(['attack'], axis=1, inplace=True)
df_kddtest.drop(['attack'], axis=1, inplace=True)

#Since 'num_outbound_cmds' fields have zero values we will delete it
df_kddtrain.drop(['num_outbound_cmds'], axis=1, inplace=True)
df_kddtest.drop(['num_outbound_cmds'], axis=1, inplace=True)

df_kddtrain

a=0
b=0
c=0
d=0
e=0
f=df_kddtrain['attack_class'].count()

for i in range(len(df_kddtrain)):
    if df_kddtrain['attack_class'][i]== 'DoS':
        a=a+1

for i in range(len(df_kddtrain)):
    if df_kddtrain['attack_class'][i]== 'Normal':
        b=b+1
        
for i in range(len(df_kddtrain)):
    if df_kddtrain['attack_class'][i]== 'Probe':
        c=c+1
        
for i in range(len(df_kddtrain)):
    if df_kddtrain['attack_class'][i]== 'R2L':
        d=d+1
        
for i in range(len(df_kddtrain)):
    if df_kddtrain['attack_class'][i]== 'U2R':
        e=e+1
        
print("Total attacks in KDDTrain= "+str(f))       
print("No of DoS attacks= "+str(a))   
print("No of Normal attacks= "+str(b))   
print("No of Probe attacks= "+str(c))   
print("No of R2L attacks= "+str(d))  
print("No of U2R attacks= "+str(e))

k=0
l=0
m=0
n=0
o=0
p=df_kddtest['attack_class'].count()

for i in range(len(df_kddtest)):
    if df_kddtest['attack_class'][i]== 'DoS':
        k=k+1

for i in range(len(df_kddtest)):
    if df_kddtest['attack_class'][i]== 'Normal':
        l=l+1
        
for i in range(len(df_kddtest)):
    if df_kddtest['attack_class'][i]== 'Probe':
        m=m+1
        
for i in range(len(df_kddtest)):
    if df_kddtest['attack_class'][i]== 'R2L':
        n=n+1
        
for i in range(len(df_kddtest)):
    if df_kddtest['attack_class'][i]== 'U2R':
        o=o+1
        
print("Total attacks in KDDTest= "+str(p))       
print("No of DoS attacks= "+str(k))   
print("No of Normal attacks= "+str(l))   
print("No of Probe attacks= "+str(m))   
print("No of R2L attacks= "+str(n))  
print("No of U2R attacks= "+str(o))

data = [[a, a/f*100,k,k/p*100], [b, b/f*100,l,l/p*100], [c,c/f*100,m,m/p*100],[d, d/f*100,n,n/p*100], [e, e/f*100,o,o/p*100]] 
#df = pd.DataFrame(data, columns = ['attack_train','freq%train','attack_test','freq%test'])
df_distribution = pd.DataFrame(data, index =['DoS', 'Normal', 'Probe', 'R2L','U2R']) 
df_distribution.columns=['attack_train','freq%train','attack_test','freq%test']
df_distribution

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib
import matplotlib.pyplot as plt

plot = df_distribution[['freq%train', 'freq%test']].plot(kind="bar");
plot.set_title("Attack Class Distribution", fontsize=20);
plot.grid(color='lightgray', alpha=0.5);

import seaborn as sns
import sklearn
import imblearn

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

cols = df_kddtrain.select_dtypes(include=['float64','int64']).columns
sc_train = scaler.fit_transform(df_kddtrain.select_dtypes(include=['float64','int64']))
sc_test = scaler.fit_transform(df_kddtest.select_dtypes(include=['float64','int64']))

sc_traindf = pd.DataFrame(sc_train, columns = cols)
sc_testdf = pd.DataFrame(sc_test, columns = cols)

sc_traindf

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

# extract categorical attributes from both training and test sets 
cattrain = df_kddtrain.select_dtypes(include=['object']).copy()
cattest = df_kddtest.select_dtypes(include=['object']).copy()

# encode the categorical attributes
traincat = cattrain.apply(encoder.fit_transform)
testcat = cattest.apply(encoder.fit_transform)

# separate target column from encoded data 
enctrain = traincat.drop(['attack_class'], axis=1)
enctest = testcat.drop(['attack_class'], axis=1)

cat_Ytrain = traincat[['attack_class']].copy()
cat_Ytest = testcat[['attack_class']].copy()

from imblearn.over_sampling import RandomOverSampler 
from collections import Counter

sc_traindf = df_kddtrain.select_dtypes(include=['float64','int64'])
refclasscol = pd.concat([sc_traindf, enctrain], axis=1).columns
refclass = np.concatenate((sc_train, enctrain.values), axis=1)
X = refclass

#Converting 2d array cat_Y to 1d array y_test/train
c, r = cat_Ytest.values.shape
y_test = cat_Ytest.values.reshape(c,)

c, r = cat_Ytrain.values.shape
y = cat_Ytrain.values.reshape(c,)

#&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
ros = RandomOverSampler(random_state=42)
X_res, y_res = ros.fit_sample(X, y)

y_res

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier();
rfc.fit(X_res, y_res);

score = np.round(rfc.feature_importances_,3)

importances = pd.DataFrame({'feature':refclasscol,'importance':score})
importances = importances.sort_values('importance',ascending=False).set_index('feature')
# plot importances
plt.rcParams['figure.figsize'] = (11, 4)
importances.plot.bar();

from sklearn.feature_selection import RFE
import itertools
rfc = RandomForestClassifier()

# create the RFE model and select 10 attributes
rfe = RFE(rfc, n_features_to_select=10)
rfe = rfe.fit(X_res, y_res)

# summarize the selection of the attributes
feature_map = [(i, v) for i, v in itertools.zip_longest(rfe.get_support(), refclasscol)]
selected_features = [v for i, v in feature_map if i==True]

selected_features

# define columns to new dataframe
newcol = list(refclasscol)
newcol.append('attack_class')

# add a dimension to target
new_y_res = y_res[:, np.newaxis]

# create a dataframe from sampled data
res_arr = np.concatenate((X_res, new_y_res), axis=1)
res_df = pd.DataFrame(res_arr, columns = newcol) 

# create test dataframe
reftest = pd.concat([sc_testdf, testcat], axis=1)
reftest['attack_class'] = reftest['attack_class'].astype(np.float64)
reftest['protocol_type'] = reftest['protocol_type'].astype(np.float64)
reftest['flag'] = reftest['flag'].astype(np.float64)
reftest['service'] = reftest['service'].astype(np.float64)

res_df.shape
reftest.shape

reftest.hist(bins=50,figsize=(25,25))

from collections import defaultdict
classdict = defaultdict(list)

# create two-target classes (normal class and an attack class)  
attacklist = [('DoS', 0.0), ('Probe', 2.0), ('R2L', 3.0), ('U2R', 4.0)]
normalclass = [('Normal', 1.0)]

def create_classdict():
    '''This function subdivides train and test dataset into two-class attack labels''' 
    for j, k in normalclass: 
        for i, v in attacklist: 
            restrain_set = res_df.loc[(res_df['attack_class'] == k) | (res_df['attack_class'] == v)]
            classdict[j +'_' + i].append(restrain_set)
            # test labels
            reftest_set = reftest.loc[(reftest['attack_class'] == k) | (reftest['attack_class'] == v)]
            classdict[j +'_' + i].append(reftest_set)
    
    print(classdict)
        
create_classdict()

for k, v in classdict.items():
    k

pretrain = classdict['Normal_DoS'][0]
pretest = classdict['Normal_DoS'][1]
grpclass = 'Normal_DoS'

pretrain_Probe = classdict['Normal_Probe'][0]
pretest_Probe = classdict['Normal_Probe'][1]
grpclass_Probe = 'Normal_Probe'

pretrain_R2L = classdict['Normal_R2L'][0]
pretest_R2L = classdict['Normal_R2L'][1]
grpclass_R2L = 'Normal_R2L'

pretrain_U2R = classdict['Normal_U2R'][0]
pretest_U2R = classdict['Normal_U2R'][1]
grpclass_U2R = 'Normal_U2R'


def modelPrediction(pretrain, pretest, grpclass):
    
    from sklearn.preprocessing import OneHotEncoder
    enc = OneHotEncoder()

    Xresdf = pretrain 
    newtest = pretest

    Xresdfnew = Xresdf[selected_features]
    Xresdfnum = Xresdfnew.drop(['service'], axis=1)
    Xresdfcat = Xresdfnew[['service']].copy()

    Xtest_features = newtest[selected_features]
    Xtestdfnum = Xtest_features.drop(['service'], axis=1)
    Xtestcat = Xtest_features[['service']].copy()

    enc.fit(Xresdfcat)

    Xtestcat

    Xtestcat=Xtestcat.reset_index()
    del Xtestcat['index']

    Xtestcat

    Xtestcat.to_numpy()

    X_train_1hotenc = enc.transform(Xresdfcat).toarray()
        
    # Transform test data
    X_test_1hotenc = Xtestcat.to_numpy()

    X_train = np.concatenate((Xresdfnum.values, X_train_1hotenc), axis=1)
    X_test = np.concatenate((Xtestdfnum.values, X_test_1hotenc), axis=1) 

    y_train = Xresdf[['attack_class']].copy()
    c, r = y_train.values.shape
    Y_train = y_train.values.reshape(c,)

    y_test = newtest[['attack_class']].copy()
    c, r = y_test.values.shape
    Y_test = y_test.values.reshape(c,)

    y_train

    from sklearn.svm import SVC 
    from sklearn.naive_bayes import BernoulliNB 
    from sklearn import tree
    from sklearn.model_selection import cross_val_score
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.ensemble import VotingClassifier

    # Train KNeighborsClassifier Model
    KNN_Classifier = KNeighborsClassifier(n_jobs=-1)
    KNN_Classifier.fit(X_train, Y_train); 

    # Train LogisticRegression Model
    LGR_Classifier = LogisticRegression(n_jobs=-1, random_state=0)
    LGR_Classifier.fit(X_train, Y_train);

    # Train Gaussian Naive Baye Model
    BNB_Classifier = BernoulliNB()
    BNB_Classifier.fit(X_train, Y_train)
                
    # Train Decision Tree Model
    DTC_Classifier = tree.DecisionTreeClassifier(criterion='entropy', random_state=0)
    DTC_Classifier.fit(X_train, Y_train);

    from sklearn import metrics
    models = []
    #models.append(('SVM Classifier', SVC_Classifier))
    models.append(('Naive Baye Classifier', BNB_Classifier))
    models.append(('Decision Tree Classifier', DTC_Classifier))
    #models.append(('RandomForest Classifier', RF_Classifier))
    models.append(('KNeighborsClassifier', KNN_Classifier))
    models.append(('LogisticRegression', LGR_Classifier))
    #models.append(('VotingClassifier', VotingClassifier))

    for i, v in models:
        scores = cross_val_score(v, X_train, Y_train, cv=10)
        accuracy = metrics.accuracy_score(Y_train, v.predict(X_train))
        confusion_matrix = metrics.confusion_matrix(Y_train, v.predict(X_train))
        classification = metrics.classification_report(Y_train, v.predict(X_train))
        print()
        print('============================== {} {} Model Evaluation =============================='.format(grpclass, i))
        print()
        print ("Cross Validation Mean Score:" "\n", scores.mean())
        print()
        print ("Model Accuracy:" "\n", accuracy)
        print()
        print("Confusion matrix:" "\n", confusion_matrix)
        print()
        print("Classification report:" "\n", classification) 
        print()

    # This will be our selected algo which will be the model.

    #DTC_Classifier = tree.DecisionTreeClassifier(criterion='entropy', random_state=0)
    #DTC_Classifier.fit(X_train, Y_train);
    Xnew = X_train
    ynew = DTC_Classifier.predict(Xnew)
    print("X=%s, Predicted=%s" % (Xnew[3], ynew[4]))

    print("X=%s, Predicted=%s" % (Xnew[0], ynew[0]))

    if ynew[0] == 1.0:
        print('Alarm it')


modelPrediction(pretrain, pretest, grpclass)
modelPrediction(pretrain_Probe, pretest_Probe, grpclass_Probe)
modelPrediction(pretrain_R2L, pretest_R2L, grpclass_R2L)
modelPrediction(pretrain_U2R, pretest_U2R, grpclass_U2R)